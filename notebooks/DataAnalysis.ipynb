{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "â€‹\n",
    "Before jumping into the task of classifying documents into classes, we should always look at the underlying data. We will convert the data into a pandas DataFrame to look at the data easily.\n",
    "\n",
    "The main fields are:\n",
    "- `document_id`: The id of the document.\n",
    "- `content`: The raw textual content of the document.\n",
    "- `labels`: The labels of the document as a list of strings.\n",
    "- `train?`: Whether the document is in the training set. This is known by a prefix in the id of the document, which is defined by the `reuters.fileids()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import pandas as pd\n",
    "\n",
    "def dataset_to_df():\n",
    "    contents = []\n",
    "    labels = []\n",
    "    is_train = []\n",
    "\n",
    "    document_ids = reuters.fileids()\n",
    "\n",
    "    # Collect data for each document\n",
    "    for doc_id in document_ids:\n",
    "        contents.append(reuters.raw(doc_id))\n",
    "        labels.append(reuters.categories(doc_id))\n",
    "        is_train.append(doc_id.startswith(\"train\"))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df_data = pd.DataFrame({\n",
    "        'document_id': document_ids,\n",
    "        'content': contents,\n",
    "        'labels': labels,\n",
    "        'train?': is_train\n",
    "    })\n",
    "    return df_data\n",
    "\n",
    "def print_document(doc_id):\n",
    "    print(\n",
    "        f\"\"\"Doc ID: {doc_id}\"\"\" \\\n",
    "        \"\\n---------------------\" \\\n",
    "        f\"\"\"\\nLABELS: {reuters.categories(doc_id)}\"\"\" \\\n",
    "        \"\\n---------------------\" \\\n",
    "        \"\\nCONTENT:\" \\\n",
    "        f\"\"\"\\n{reuters.raw(doc_id)}\"\"\" \\\n",
    "        \"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = dataset_to_df()\n",
    "\n",
    "# How doocuments look like?\n",
    "print_document('training/9865')\n",
    "\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Stats\n",
    "print(f\"Total documents: {len(df_data)}\")\n",
    "print(f\"Total train documents: {len(df_data[df_data['train?'] == True])}\")\n",
    "print(f\"Total test documents: {len(df_data[df_data['train?'] == False])}\")\n",
    "print(f\"Total labels: {len(df_data['labels'].explode().unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "# Category distribution\n",
    "print(\"\\nCategory frequencies:\")\n",
    "category_counts = df_data['labels'].explode().value_counts()\n",
    "df_category_distribution = category_counts.to_frame(name='documents')\n",
    "sorted_data = df_category_distribution.sort_values(by='documents', ascending=False)\n",
    "sorted_data.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
